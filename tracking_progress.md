## 0Ô∏è‚É£ - Where we stand **this minute**

| Area                                 | Implementation status                           | Proof in repo                                                       |
| ------------------------------------ | ----------------------------------------------- | ------------------------------------------------------------------- |
| **DuckDB feature store**             | ‚úÖ Live ‚Äì cache-on-miss, primary-key indexed     | `core/feature_store/feature_store.py`                               |
| **Event bus** (Redis Streams)        | ‚úÖ Publishing & subscribe helpers                | `core/event_bus.py`                                                 |
| **Joint Policy** (LightGBM)          | ‚úÖ Model class + trainer + shadow inference      | `models/joint_policy.py`, `training/train_joint_policy.py`          |
| **RL stack** (Gym env + PPO trainer) | ‚úÖ Training + replay summary log flows connected | `core/rl/gym_env.py`, `core/rl/ppo_trainer.py`, `flows/train_rl.py` |
| **Policy routing**                   | ‚úÖ RL/Joint/Mix supported via config             | `core/policy/__init__.py`, `core/system_state.py`                   |
| **Sharpe-based allocation gate**     | ‚úÖ Daily cron deployed via Prefect               | `core/predict/policy_chooser.py`, `policy_chooser_deploy.yaml`      |
| **Replay enrichment**                | ‚úÖ Missed-trade, rejected-trade, holding-penalty | `agents/signal_arbitration_agent.py`, `agents/replay_logger.py`     |
| **Skiplist TTL**                     | ‚úÖ Schema & cleanup flow completed               | `core/skiplist/skiplist.py`, `flows/clean_skiplist_flow.py`         |
| **Bootstrap simulation**             | ‚úÖ Multi-phase runner with reward logging        | `bootstrap/run_full_bootstrap.py`, `bootstrap/bootstrap_trader.py`  |

---

## 1Ô∏è‚É£ üîß  **Top three jobs now**

| #     | Change                                                                                 | Why first?                                | Rough lift                                          |
| ----- | -------------------------------------------------------------------------------------- | ----------------------------------------- | --------------------------------------------------- |
| **1** | ‚úÖ **Instrument richer rewards**<br>holding-cost, slippage & regime tags in replay rows | RL needs realistic credit signals         | Done: `replay_logger.py`, `gym_env.py`              |
| **2** | ‚úÖ **Replay summary on cron + Sharpe plots**                                            | Daily training snapshot + visual feedback | `track_replay_summary_flow.py` ready; cron optional |
| **3** | ‚úÖ **Bootstrap data generation phase 0‚Äì2**                                              | Unblocks training for all policies        | `run_full_bootstrap.py` and supporting flows        |

---

## 2Ô∏è‚É£ üó∫Ô∏è  **Migration path ‚Äì week-by-week**

| Week   | Goal                                                                        | New code / config                                            | Co-existence strategy                                     |
| ------ | --------------------------------------------------------------------------- | ------------------------------------------------------------ | --------------------------------------------------------- |
| **W1** | ‚úÖ Reward instrumentation<br>‚úÖ Skiplist TTL<br>‚úÖ `policy_chooser` scheduling | `policy_chooser_deploy.yaml`, `track_replay_summary_flow.py` | Grid/Joint/RL run side-by-side; allocation adjusts daily. |
| **W2** | ‚úÖ **Bootstrap system (phase 0‚Äì2)**                                          | `bootstrap/run_full_bootstrap.py`, `bootstrap_trader.py`     | Uses past OHLCV to simulate training data                 |
| **W3** | **Shadow RL inference live** (10 % allocation cap)                          | Set `rl_allocation = 10` in system config                    | `ExecutionAgentSQL` uses chosen policy type.              |
| **W4** | **Joint + RL model distillation loop**                                      | Activate `flows/train_joint_policy_from_rl.py` job           | Joint still live; RL trained from high-confidence trades. |
| **W5** | **Retire Grid & Param fallbacks** in `StrategyAgent`                        | Disable grid search paths, simplify planning logic           | StrategyAgent chooses policy from system config.          |
| **W6** | **Parallel RL training** (vector envs or JAX SB3)                           | Use `gym.vector.SyncVectorEnv`, run multi-actor setup        | Same replay buffer, faster transitions per day.           |

---

## 3Ô∏è‚É£ üß¨  **Joint Policy evolution blueprint**

1. **Model bump**: swap LightGBM for Tiny Transformer (e.g., 4 layers √ó 128 dim) with `enter`, `size`, `exit` heads.
2. **Training**:

   * *Phase A:* ‚úÖ Clone RF/Grid trades ‚Äî done.
   * *Phase B:* ‚è≥ Daily distillation from top-RL checkpoints ‚Äî to begin Sprint 2.
3. **Serving**: TorchScript export + reload via unified `core.policy.predict()`.
4. **Retire LightGBM**: once AUC + latency thresholds are beat.

---

## 4Ô∏è‚É£ ‚öôÔ∏è  **Feature-flow verdict**

üü¢ **Stick with DuckDB** through 2025.

* Already in use by RL env, feature backfill, and Joint trainer.
* Fast, embedded, cheap to maintain.
* Next: add weekly compaction job to control DB size (< 2 GB).

Kafka or Redis pub/sub can come later for remote inference or async UI triggers.

---

## 5Ô∏è‚É£ ü§ñ  **Concrete checklist to make RL ‚Äúcaptain of the ship‚Äù**

| Component             | Required change                                                                               |
| --------------------- | --------------------------------------------------------------------------------------------- |
| **ExecutionAgentSQL** | ‚úÖ Publish `FILL`, `M2M_PNL`, `UNWIND` to Redis stream.                                        |
| **Replay buffer**     | ‚úÖ Include `holding_days`, `regime_tag`, `slippage`.                                           |
| **Gym env**           | ‚úÖ Use these to compute `reward = pnl - hold_penalty - slippage_penalty + capital_efficiency`  |
| **Trainer**           | ‚è≥ Try AWR (advantage-weighted regression) or AWAC to stabilize continuous `size` head.        |
| **Inference path**    | ‚úÖ `core.policy.predict()` routes to RL / Joint / Mix cleanly.                                 |
| **Data volume**       | ‚úÖ Bootstrap + replay logs enable >200 transitions/day, scaling to 20k+/week with vector envs. |

---

## 6Ô∏è‚É£ üìÖ  **At-a-glance sprint board**

| Sprint (2-week cadence)                  | Epic                          | Key deliverables                                                    |
| ---------------------------------------- | ----------------------------- | ------------------------------------------------------------------- |
| **Sprint 1** (‚úÖ complete)                | *Reward + skiplist + chooser* | ‚Ä¢ Holding/slip rewards live<br>‚Ä¢ Skiplist TTL<br>‚Ä¢ Sharpe dashboard |
| **Sprint 2** (‚ñ∂ now live)                | *Bootstrap + shadow trading*  | ‚Ä¢ Phase 0‚Äì2 bootstrap done<br>‚Ä¢ RL gets 10 % capital                |
| **Sprint 3** (Jun 29 ‚Üí Jul 12)           | *Model consolidation*         | ‚Ä¢ Remove Grid/Param<br>‚Ä¢ Promote Joint+RL only                      |
| **Sprint 4** (Jul 13 ‚Üí Jul 26)           | *Parallel RL & regime tags*   | ‚Ä¢ Vectorised PPO<br>‚Ä¢ Volatility regime tagging in replay           |
| **Sprint 5** (Jul 27 ‚Üí Aug 9)            | *RL majority control*         | ‚Ä¢ Allocation gate ‚â• 70 %<br>‚Ä¢ LGBM ‚Üí Tiny-Transformer prototype     |
| **Hardening / buffer** (Aug 10 ‚Üí Aug 23) | *Clean-up & docs*             | ‚Ä¢ Delete dead tables/flows<br>‚Ä¢ Full pipeline runbook               |

---

### ‚úîÔ∏è Immediate next actions

1. üîõ Phase 0‚Äì2 already running via `run_full_bootstrap.py`.
2. ‚úÖ Replay enriched with synthetic + guided trades.
3. üîú Set `rl_allocation = 10` to let RL infer on today's picks.
4. üîú Begin off-policy distillation job for JointPolicy from RL replay.
